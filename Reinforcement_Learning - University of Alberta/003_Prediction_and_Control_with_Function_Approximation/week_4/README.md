**Module 1 - Learning Parameterized Policies**  
**Module 2 - Policy Gradient for continuing Tasks**  
**Module 3 - Actor-Critic for continuing Tasks**
**Module 4 - Policy Parameterization**  
  
- Understand how to define policies as parameterized functions
- Define one class of parameterized policies based on the softmax function
- Understand the advantages of using parameterized policies over action-value based methods
- Describe the objective for policy gradient algorithms
- Describe the results of the policy gradient theorem
- Understand the importance of the policy gradient theorem
- Derive a sample-based estimate for the gradient of the average reward objective
- Describe the actor-critic algorithm for control with function approximation, for continuing tasks
- Derive the actor-critic update for a softmax policy with linear action preferences
- Implement the actor-critic algorithm with a softmax policy with linear action preferences
- Design concrete function approximators for an average reward actor-critic algorithm
- Analyze the performance of an average reward agent
- Derive the actor-critic update for a gaussian policy
- Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions
